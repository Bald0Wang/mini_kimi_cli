# 03. 探秘：解剖数字大脑 (Code Walkthrough)

> "代码是写给人看的，只是顺便让机器运行。" —— Donald Knuth

如果说前两章我们是在观察一个黑盒子的外部行为，那么这一章，我们将打开盒子，看看里面的齿轮是如何咬合的。阅读源码就像是一场探险，我们需要顺着执行流（Execution Flow）这条线索，穿越一个个模块，最终理解整个系统的运作机理。Mini Kimi 虽然小，但五脏俱全，它浓缩了现代 Agent 架构的精华。

### 知识加油站 ⛽️

#### 1. REPL (Read-Eval-Print Loop)
Mini Kimi 的交互界面本质上是一个 REPL 环境。这是一种简单的交互式编程环境：读取用户输入 (Read)，执行求值 (Eval)，打印结果 (Print)，然后循环 (Loop)。这种模式最早可以追溯到 Lisp 语言，现在被广泛用于各种 Shell 和解释器中。
*   **核心逻辑**: `while True: print(eval(read()))`

#### 2. 分层架构 (Layered Architecture)
为了不让代码变成一团乱麻，Mini Kimi 采用了分层设计：UI 层负责交互，Soul 层负责核心逻辑，LLM 层负责模型通信，Tools 层负责具体能力。这种“关注点分离”的设计原则，是构建大型软件系统的基石。
*   **参考阅读**: [Software Architecture Patterns - Layered Architecture](https://www.oreilly.com/library/view/software-architecture-patterns/9781491971437/ch01.html)

---

### 学习目标

- 能从入口一路跟到工具执行：`main.py → Soul.run → LLMClient.chat → tool_calls → tool_result → 再次 chat`
- 能画出一张“消息流转图”（messages 如何增长）
- 能定位一个行为属于哪个模块（UI / Soul / LLM / Tools）

---

### 建议阅读顺序（按调用链）

1) `mini_kimi/src/mini_kimi/ui/shell/main.py`
2) `mini_kimi/src/mini_kimi/soul/soul.py`
3) `mini_kimi/src/mini_kimi/llm/client.py`
4) `mini_kimi/src/mini_kimi/tools/*`

---

### 模块 1：入口 UI（`ui/shell/main.py`）

**你要看什么**

- REPL 循环：`input()` → `agent.run(user_input)`
- 为什么要把 `src` 放到 `sys.path`（让 `mini_kimi` 包可被导入）

**检查点**

- 你能解释：用户输入是从哪里进入系统的？最终调用的是哪个方法？

**技术细节**

- 入口的职责应该尽量“薄”：只处理输入/输出，不做业务逻辑
- 入口里如果出现大量 Agent 逻辑，后续会难以维护（这是分层的意义）

---

### 模块 2：核心循环（`soul/soul.py`）

**你要看什么**

- `self.messages` 的初始 system prompt
- 主循环里对 `response_msg.tool_calls` 的判断
- 工具执行完后如何回填为 `role: tool`

**建议你在本地做一个观察**

在 `Soul.run()` 里加一行打印（调试用）：

- 打印 `len(self.messages)`，并在每轮循环打印一次

**检查点**

- 你能指出：工具结果是以什么结构写回 `self.messages` 的（包含哪些字段）？

**关键数据结构（你应该能在代码里找到对应字段）**

工具结果写回时（概念结构）：

```json
{
  "role": "tool",
  "tool_call_id": "模型返回的调用 id",
  "name": "工具名，例如 SearchWeb",
  "content": "工具输出（通常会截断）"
}
```

为什么必须写回？

- 因为模型下一轮推理只看 `messages`
- 如果你只打印在终端但不写回，模型会“看不到结果”，无法基于结果继续推理

---

### 模块 3：模型调用（`llm/client.py`）

**你要看什么**

- `get_api_key()` 的优先级：环境变量 → 配置文件 → 交互输入（如实现存在）
- `LLMClient.chat()` 如何把 `tools` 的 schema 传给接口

**检查点**

- 你能解释：`tools` 参数传给接口后，返回的 `tool_calls` 结构是什么样？

**技术细节（建议你对照 SDK 返回对象）**

- 返回通常包含 `message.tool_calls`
- 每个 tool call 里有 `function.name` 与 `function.arguments`（arguments 是 JSON 字符串）

---

### 模块 4：工具实现（`tools/`）

**你要看什么**

- 每个工具的 `schema`：名字、描述、参数
- 每个工具的 `__call__`：真正执行
- 工具输出为什么要做截断（避免塞爆上下文/终端）

**检查点**

- 你能解释：`schema["function"]["name"]` 为什么必须与 `Soul` 里的路由名称一致？

---

### 验证方式（用一次真实任务串起来）

建议用这个任务跑一遍，边跑边对照代码：

- 输入：`搜索 ddgs 的用法，并读取一个结果网页后给出 5 条要点`

你应当能观察到：

- 入口把输入交给 `Soul.run()`
- `LLMClient.chat()` 返回 `tool_calls`
- `SearchWeb` 执行并写回 `role: tool`
- 下一轮可能调用 `FetchURL`
- 最后模型返回 `assistant` 的总结

---

### 本节小结

- **你掌握了什么**：从入口到工具执行的完整调用链
- **你理解了什么**：为什么“写回 messages”是 Agent 可持续推理的关键
